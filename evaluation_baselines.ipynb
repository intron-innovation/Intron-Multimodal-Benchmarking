{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e245181",
   "metadata": {},
   "source": [
    "\n",
    "# üß™ AfriHealth-MultiBench Evaluation Notebook\n",
    "\n",
    "This notebook provides **baseline evaluation scripts** for the AfriHealth-MultiBench project (or any similar multimodal benchmark).  \n",
    "It evaluates performance across:\n",
    "\n",
    "- üó£Ô∏è **Speech Recognition (WER)**\n",
    "- üåç **Translation (BLEU, ChrF)**\n",
    "- ‚ùì **Question Answering (Exact Match, F1)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# 1. Setup & Imports\n",
    "# ================================================================\n",
    "\n",
    "!pip install jiwer sacrebleu evaluate pandas numpy tqdm --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer\n",
    "import sacrebleu\n",
    "import evaluate\n",
    "\n",
    "# Load Hugging Face SQuAD evaluation metric\n",
    "qa_metric = evaluate.load(\"squad\")\n",
    "\n",
    "print(\"‚úÖ Environment ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# 2. Load Predictions & References\n",
    "# ================================================================\n",
    "\n",
    "# Expected CSV format:\n",
    "# ASR: id | reference | hypothesis\n",
    "# MT:  id | source | reference | prediction\n",
    "# QA:  id | context | question | reference | prediction\n",
    "\n",
    "# Example file paths (replace with your data paths)\n",
    "asr_df = pd.read_csv(\"data/asr_results.csv\")\n",
    "mt_df = pd.read_csv(\"data/translation_results.csv\")\n",
    "qa_df = pd.read_csv(\"data/qa_results.csv\")\n",
    "\n",
    "print(\"ASR:\", asr_df.shape)\n",
    "print(\"MT:\", mt_df.shape)\n",
    "print(\"QA:\", qa_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db514ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# 3. Evaluation Functions\n",
    "# ================================================================\n",
    "\n",
    "# 3a. Word Error Rate (WER)\n",
    "def evaluate_asr(df):\n",
    "    wers = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        wers.append(wer(row[\"reference\"], row[\"hypothesis\"]))\n",
    "    avg_wer = np.mean(wers)\n",
    "    return {\"WER\": round(avg_wer * 100, 2)}\n",
    "\n",
    "\n",
    "# 3b. Translation (BLEU, ChrF)\n",
    "def evaluate_translation(df):\n",
    "    references = [[ref] for ref in df[\"reference\"].tolist()]\n",
    "    predictions = df[\"prediction\"].tolist()\n",
    "    \n",
    "    bleu = sacrebleu.corpus_bleu(predictions, references).score\n",
    "    chrf = sacrebleu.corpus_chrf(predictions, references).score\n",
    "    \n",
    "    return {\"BLEU\": round(bleu, 2), \"ChrF\": round(chrf, 2)}\n",
    "\n",
    "\n",
    "# 3c. Question Answering (Exact Match, F1)\n",
    "def evaluate_qa(df):\n",
    "    predictions = [{\"id\": str(r[\"id\"]), \"prediction_text\": r[\"prediction\"]} for _, r in df.iterrows()]\n",
    "    references = [{\"id\": str(r[\"id\"]), \"answers\": {\"text\": [r[\"reference\"]], \"answer_start\": [0]}} for _, r in df.iterrows()]\n",
    "    \n",
    "    results = qa_metric.compute(predictions=predictions, references=references)\n",
    "    return {\"ExactMatch\": round(results[\"exact_match\"], 2), \"F1\": round(results[\"f1\"], 2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e513d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# 4. Run Evaluation\n",
    "# ================================================================\n",
    "\n",
    "asr_scores = evaluate_asr(asr_df)\n",
    "mt_scores = evaluate_translation(mt_df)\n",
    "qa_scores = evaluate_qa(qa_df)\n",
    "\n",
    "# Combine into summary\n",
    "summary = pd.DataFrame([asr_scores, mt_scores, qa_scores], index=[\"ASR\", \"MT\", \"QA\"])\n",
    "\n",
    "print(\"\\nüìä Evaluation Summary:\\n\")\n",
    "display(summary)\n",
    "\n",
    "# Save results\n",
    "summary.to_csv(\"evaluation_summary.csv\", index=True)\n",
    "print(\"\\n‚úÖ Saved evaluation summary to 'evaluation_summary.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b42a7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üìÑ Example Input Format\n",
    "\n",
    "**ASR**\n",
    "```csv\n",
    "id,reference,hypothesis\n",
    "1,thank you for calling,thank you for calling\n",
    "2,how are you doing today,how are doing today\n",
    "```\n",
    "\n",
    "**Translation**\n",
    "```csv\n",
    "id,source,reference,prediction\n",
    "1,bonjour,hello,hi\n",
    "2,merci beaucoup,thank you very much,thanks a lot\n",
    "```\n",
    "\n",
    "**Question Answering**\n",
    "```csv\n",
    "id,context,question,reference,prediction\n",
    "1,The capital of Nigeria is Abuja,What is the capital of Nigeria?,Abuja,Lagos\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8aaa8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
